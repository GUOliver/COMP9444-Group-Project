{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b64af076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "22954ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dce5be",
   "metadata": {},
   "source": [
    "### Read data backout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eb655666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdf(file_path):\n",
    "    return_dict = []\n",
    "    with h5py.File(file_path, 'r') as hf:\n",
    "        print(list(hf.keys()))\n",
    "        print(file_path[9:-5])\n",
    "        dataset = hf[file_path[9:-5]]\n",
    "        print(len(list(dataset.keys())))\n",
    "        keys = list(dataset.keys())\n",
    "        for i in keys:\n",
    "            return_dict.append({\n",
    "                \"name\":i.replace(\"-\",\"/\"),\n",
    "                \"data\":dataset[i][:]\n",
    "            })\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8ec0a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = ['Experimental', 'Electronic', 'Rock', 'Instrumental', 'Pop', 'Folk', 'Hip-Hop', 'International']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1c755baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip mel_spec/update_feature.zip\n",
    "# Move all files into mel_sepc then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f287554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Experimental']\n",
      "Experimental\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "hdf5_dict[\"Experimental\"] = read_hdf(\"mel_spec/Experimental.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "16ef1348",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec_base_path = \"mel_spec/\"\n",
    "for i in genre:\n",
    "    output = read_hdf(mel_spec_base_path+i+\".hdf5\") # list of dictionaries\n",
    "    hdf5_dict[i] = output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a3969c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 646)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf5_dict[\"International\"][0][\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ec1ce100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_reference = {}\n",
    "# for index,value in enumerate(hdf5_dict.keys()):\n",
    "#     label_reference[value] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1830336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_reference = genres_dict = {\n",
    "    'Electronic': 0, \n",
    "    'Experimental': 1, \n",
    "    'Folk': 2, \n",
    "    'Hip-Hop': 3, \n",
    "    'Instrumental': 4,\n",
    "    'International': 5, \n",
    "    'Pop': 6, \n",
    "    'Rock': 7  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bcfac83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting_test_valid(dictionary,test_size,valid_size):\n",
    "    x = []\n",
    "    y = []\n",
    "    for genres,datas in dictionary.items():\n",
    "        for i in datas:\n",
    "            x.append(i['data'])\n",
    "            y.append(label_reference[genres])\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = test_size)\n",
    "    x_train,x_valid,y_train,y_valid = train_test_split(x_train,y_train,test_size = valid_size)\n",
    "    return x_train,x_valid,x_test,y_train,y_valid,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "16638b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_valid,x_test,y_train,y_valid,y_test = splitting_test_valid(hdf5_dict,0.2,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f8e23429",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = np.rollaxis(np.dstack(x_train),-1)\n",
    "x_valid = np.rollaxis(np.dstack(x_valid),-1)\n",
    "x_test = np.rollaxis(np.dstack(x_test),-1)\n",
    "y_train = np.rollaxis(np.dstack(y_train),-1)\n",
    "y_valid = np.rollaxis(np.dstack(y_valid),-1)\n",
    "y_test = np.rollaxis(np.dstack(y_test),-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5ac3e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[...,np.newaxis]\n",
    "x_valid = x_valid[...,np.newaxis]\n",
    "x_test = x_test[...,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f8337ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5116, 128, 646, 1)\n",
      "(1279, 128, 646, 1)\n",
      "(1599, 128, 646, 1)\n",
      "(5116, 1, 1)\n",
      "(1279, 1, 1)\n",
      "(1599, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600497d",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "97a97e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Since the task is not overly ccomplex, choose to use LeNet 5\n",
    "'''\n",
    "from keras import layers\n",
    "input_shape = (x_train.shape[1], x_train.shape[2], x_train.shape[3])\n",
    "# inputs = keras.Input(shape=input_shape) \n",
    "\n",
    "def model(input_shape,label_reference):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "    model.add(tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(128,activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(len(label_reference), activation='softmax'))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ea01621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(input_shape,label_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f538d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate = 0.0005)\n",
    "model.compile(optimizer = optimizer, loss = \"SparseCategoricalCrossentropy\",metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b60e2b4",
   "metadata": {},
   "source": [
    "### Fitting and training the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "aced8726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1279/1279 [==============================] - 158s 123ms/step - loss: 2.2853 - accuracy: 0.2353 - val_loss: 2.2745 - val_accuracy: 0.3229\n",
      "Epoch 2/10\n",
      "1279/1279 [==============================] - 164s 128ms/step - loss: 2.0047 - accuracy: 0.3223 - val_loss: 2.1041 - val_accuracy: 0.3182\n",
      "Epoch 3/10\n",
      "1279/1279 [==============================] - 161s 126ms/step - loss: 1.8213 - accuracy: 0.3853 - val_loss: 3.8077 - val_accuracy: 0.2737\n",
      "Epoch 4/10\n",
      "1279/1279 [==============================] - 188s 147ms/step - loss: 1.7310 - accuracy: 0.4097 - val_loss: 1.9523 - val_accuracy: 0.3995\n",
      "Epoch 5/10\n",
      "1279/1279 [==============================] - 190s 149ms/step - loss: 1.6480 - accuracy: 0.4328 - val_loss: 1.9483 - val_accuracy: 0.4011\n",
      "Epoch 6/10\n",
      "1279/1279 [==============================] - 171s 134ms/step - loss: 1.5350 - accuracy: 0.4734 - val_loss: 2.3703 - val_accuracy: 0.3831\n",
      "Epoch 7/10\n",
      "1279/1279 [==============================] - 162s 127ms/step - loss: 1.3678 - accuracy: 0.5323 - val_loss: 1.7582 - val_accuracy: 0.4285\n",
      "Epoch 8/10\n",
      "1279/1279 [==============================] - 174s 136ms/step - loss: 1.2623 - accuracy: 0.5610 - val_loss: 1.7136 - val_accuracy: 0.4832\n",
      "Epoch 9/10\n",
      "1279/1279 [==============================] - 193s 151ms/step - loss: 1.1426 - accuracy: 0.6020 - val_loss: 1.8977 - val_accuracy: 0.4246\n",
      "Epoch 10/10\n",
      "1279/1279 [==============================] - 186s 146ms/step - loss: 1.0286 - accuracy: 0.6478 - val_loss: 1.7423 - val_accuracy: 0.4965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c14019e20>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,validation_data = (x_valid,y_valid),epochs=10,batch_size = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c15f3",
   "metadata": {},
   "source": [
    "### Checking the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e9a4502c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 357ms/step - loss: 0.4062 - accuracy: 0.7500\n",
      "Error is 0.40623772144317627 and accuracy is 0.75\n"
     ]
    }
   ],
   "source": [
    "error,accuracy = model.evaluate(x_test[0:4],y_test[0:4],verbose = 1)\n",
    "print(f\"Error is {error} and accuracy is {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
